Tokenization using Byte Pair Encoding (BPE) is a popular method for splitting text into subword units. This technique is particularly useful for handling rare words, out-of-vocabulary words, and languages with large vocabularies.

Here, we'll implement a simplified version of BPE tokenization, called miniBPE. This includes the following steps:

1. **Build the vocabulary from a corpus**: Create a list of all unique words and their frequencies.
2. **Initialize subword units**: Start with individual characters as subword units.
3. **Merge frequent pairs of subwords**: Iteratively merge the most frequent pairs of subwords until the desired vocabulary size is reached.
4. **Encode text using the learned subword units**: Convert text into a sequence of subword units based on the learned merges.

### Implementation

#### Step 1: Build the Vocabulary

We'll start by building the vocabulary from a given corpus.

```python
from collections import Counter, defaultdict

def build_vocab(corpus):
    vocab = Counter()
    for sentence in corpus:
        words = sentence.strip().split()
        for word in words:
            # Append the end-of-word marker
            vocab[' '.join(list(word)) + ' </w>'] += 1
    return vocab

corpus = [
    "low lower lowest",
    "newest newer new"
]

vocab = build_vocab(corpus)
print("Initial Vocabulary:", vocab)
```

#### Step 2: Initialize Subword Units

Each word is split into characters, with a special end-of-word marker `</w>` added to indicate the end of a word.

#### Step 3: Merge Frequent Pairs of Subwords

Iteratively merge the most frequent pairs of subwords until the desired vocabulary size is reached.

```python
def get_stats(vocab):
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i + 1])] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)
    for word in v_in:
        w_out = word.replace(bigram, replacement)
        v_out[w_out] = v_in[word]
    return v_out

num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f'Merge {i + 1}: {best}')
    print("Vocabulary:", vocab)
```

#### Step 4: Encode Text Using the Learned Subword Units

Convert text into a sequence of subword units based on the learned merges.

```python
def encode(word, bpe_codes):
    word = ' '.join(list(word)) + ' </w>'
    symbols = word.split()
    while True:
        pairs = [(symbols[i], symbols[i + 1]) for i in range(len(symbols) - 1)]
        if not pairs:
            break
        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float('inf')))
        if bigram not in bpe_codes:
            break
        first, second = bigram
        new_symbols = []
        i = 0
        while i < len(symbols):
            try:
                j = symbols.index(first, i)
                new_symbols.extend(symbols[i:j])
                i = j
            except ValueError:
                new_symbols.extend(symbols[i:])
                break

            if symbols[i] == first and i < len(symbols) - 1 and symbols[i + 1] == second:
                new_symbols.append(first + second)
                i += 2
            else:
                new_symbols.append(symbols[i])
                i += 1
        symbols = new_symbols
    return ' '.join(symbols).replace(' </w>', '')

bpe_codes = {pair: i for i, pair in enumerate(get_stats(vocab).keys())}

word = "newest"
encoded_word = encode(word, bpe_codes)
print(f'Encoded word: {encoded_word}')
```

### Explanation

1. **Build the Vocabulary**: The `build_vocab` function processes the corpus, splitting each word into characters and adding an end-of-word marker. It returns a dictionary with subwords as keys and their frequencies as values.

2. **Initialize Subword Units**: Each word is initially represented as a sequence of characters.

3. **Merge Frequent Pairs of Subwords**:
    - The `get_stats` function computes the frequency of each pair of adjacent subwords.
    - The `merge_vocab` function performs the merge of the most frequent pair, updating the vocabulary accordingly.
    - We perform a fixed number of merges (controlled by `num_merges`).

4. **Encode Text Using the Learned Subword Units**:
    - The `encode` function converts a word into a sequence of subword units based on the learned BPE codes.
    - It applies the most frequent merges first, progressively combining characters into subwords.

### Conclusion

This implementation provides a basic version of the BPE tokenization algorithm. In practice, libraries like Hugging Face's `tokenizers` offer optimized and more feature-rich implementations. However, understanding this foundational process helps in grasping how subword tokenization works, which is crucial for building and utilizing modern NLP models like GPT-2.