Backpropagation is a key algorithm used in training neural networks and is central to most modern machine learning models. It’s a method for optimizing neural networks by minimizing the error between predicted and actual values through gradient descent. Here's a step-by-step explanation of how backpropagation works:

### 1. **Forward Pass**

In the forward pass, the input data is fed through the network to produce an output. Here’s what happens during this phase:

1. **Input Layer**: The input features are provided to the network.
2. **Hidden Layers**: Each hidden layer performs a weighted sum of the inputs and applies an activation function (like ReLU, Sigmoid, or Tanh).
3. **Output Layer**: The final layer produces the output, which could be a class label in classification tasks or a continuous value in regression tasks.

### 2. **Compute Loss**

After obtaining the output from the forward pass, the next step is to compute the loss (or error). The loss function quantifies the difference between the predicted output and the actual target value. Common loss functions include:

- **Mean Squared Error (MSE)** for regression tasks
- **Cross-Entropy Loss** for classification tasks

### 3. **Backward Pass (Backpropagation)**

The backward pass involves calculating gradients of the loss function with respect to each weight in the network and updating the weights to minimize the loss. This is done in two main steps:

#### a. **Compute Gradients**

1. **Calculate Gradients**: Use the chain rule of calculus to compute the gradients of the loss function with respect to each weight in the network. This involves:
   - Calculating the gradient of the loss with respect to the output.
   - Propagating this gradient backward through the network, layer by layer, using the chain rule.
   - For each weight, compute how much it contributed to the loss and adjust accordingly.

2. **Update Weights**: Once gradients are computed, update each weight using gradient descent. The weights are adjusted by subtracting a fraction of the gradient (scaled by the learning rate).

#### b. **Gradient Descent**

1. **Learning Rate**: Choose a learning rate that determines the step size in weight updates. Too high a learning rate can lead to overshooting, while too low can make the training slow.
2. **Update Rule**: The weights are updated as follows:
  $$
   W = W - \eta \frac{\partial L}{\partial W}
  $$
   Where:
   -$ W$ is the weight.
   -$ \eta$ is the learning rate.
   -$ \frac{\partial L}{\partial W}$ is the gradient of the loss$ L$ with respect to the weight$ W$.

### 4. **Repeat**

The forward pass, loss computation, and backward pass are repeated for multiple epochs (iterations) over the training data until the network converges and the loss is minimized.

### Example in Python

Here’s a simplified example of backpropagation in a neural network using Python. This example assumes a basic neural network with one hidden layer:

```python
import numpy as np

# Activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Training data
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([[0], [1], [1], [0]])

# Initialize weights
np.random.seed(1)
input_layer_size = 2
hidden_layer_size = 4
output_layer_size = 1

weights_input_hidden = np.random.rand(input_layer_size, hidden_layer_size)
weights_hidden_output = np.random.rand(hidden_layer_size, output_layer_size)

learning_rate = 0.5
epochs = 10000

# Training
for epoch in range(epochs):
    # Forward pass
    hidden_layer_input = np.dot(inputs, weights_input_hidden)
    hidden_layer_output = sigmoid(hidden_layer_input)
    
    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)
    output_layer_output = sigmoid(output_layer_input)
    
    # Compute loss
    loss = targets - output_layer_output
    
    # Backward pass
    output_layer_error = loss
    output_layer_delta = output_layer_error * sigmoid_derivative(output_layer_output)
    
    hidden_layer_error = np.dot(output_layer_delta, weights_hidden_output.T)
    hidden_layer_delta = hidden_layer_error * sigmoid_derivative(hidden_layer_output)
    
    # Update weights
    weights_hidden_output += np.dot(hidden_layer_output.T, output_layer_delta) * learning_rate
    weights_input_hidden += np.dot(inputs.T, hidden_layer_delta) * learning_rate
    
    if epoch % 1000 == 0:
        print(f'Epoch {epoch}, Loss: {np.mean(np.abs(loss))}')

print("Training complete.")
print("Final output:\n", output_layer_output)
```

### Key Points

1. **Forward Pass**: Calculate outputs based on current weights.
2. **Compute Loss**: Measure how far off the network's predictions are from actual values.
3. **Backward Pass**: Compute gradients and adjust weights.
4. **Training Loop**: Repeat the process until the network converges.

This example illustrates the basic principles of backpropagation. In practice, neural networks use more complex architectures, optimizers (like Adam or RMSprop), and techniques to improve efficiency and performance.


Micrograd is a minimalistic implementation of a machine learning library focused on automatic differentiation and backpropagation. It is often used as an educational tool to demonstrate the principles of building neural networks from scratch. Let's walk through a basic example to understand how Micrograd works.

### Overview of Micrograd

Micrograd allows you to define simple neural networks and perform forward and backward passes. The library implements basic components such as:

1. **Scalar class**: Represents a single value with support for gradients.
2. **Neural network layers**: Basic building blocks like neurons and layers.
3. **Backpropagation**: Automatic differentiation to compute gradients.

### Step-by-Step Implementation

#### Step 1: Scalar Class

The `Scalar` class represents a value and its gradient. It supports basic arithmetic operations and tracks the computational graph.

```python
class Scalar:
    def __init__(self, value, _children=(), _op=''):
        self.value = value
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op

    def __add__(self, other):
        out = Scalar(self.value + other.value, (self, other), '+')
        
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        
        return out

    def __mul__(self, other):
        out = Scalar(self.value * other.value, (self, other), '*')
        
        def _backward():
            self.grad += other.value * out.grad
            other.grad += self.value * out.grad
        out._backward = _backward
        
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()
```

#### Step 2: Neuron and Layer Classes

Neurons and layers are the basic components of a neural network. Each neuron computes a weighted sum of its inputs and applies a non-linear activation function.

```python
import random

class Neuron:
    def __init__(self, nin):
        self.w = [Scalar(random.uniform(-1, 1)) for _ in range(nin)]
        self.b = Scalar(0)
    
    def __call__(self, x):
        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)
        out = act.tanh()
        return out

class Layer:
    def __init__(self, nin, nout):
        self.neurons = [Neuron(nin) for _ in range(nout)]
    
    def __call__(self, x):
        outs = [neuron(x) for neuron in self.neurons]
        return outs if len(outs) > 1 else outs[0]

class MLP:
    def __init__(self, nin, nouts):
        sz = [nin] + nouts
        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]
    
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
```

#### Step 3: Training the Model

To train the model, we need a loss function, forward pass, backward pass, and an update step.

```python
# Sample dataset
data = [
    ([2.0, 3.0], 1.0),
    ([1.0, 2.0], 0.0),
    ([2.0, 2.0], 1.0),
    ([3.0, 3.0], 0.0)
]

# Define the model
model = MLP(2, [4, 4, 1])

# Training loop
learning_rate = 0.01
for epoch in range(100):
    # Forward pass
    loss = Scalar(0)
    for x, y in data:
        pred = model(x)[0]
        loss += (pred - Scalar(y)) ** 2

    # Backward pass
    loss.backward()
    
    # Update weights
    for p in model.parameters():
        p.value -= learning_rate * p.grad

    # Reset gradients
    for p in model.parameters():
        p.grad = 0.0

    print(f"Epoch {epoch}: Loss = {loss.value}")
```

### Complete Code

```python
class Scalar:
    def __init__(self, value, _children=(), _op=''):
        self.value = value
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op

    def __add__(self, other):
        out = Scalar(self.value + other.value, (self, other), '+')
        
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        
        return out

    def __mul__(self, other):
        out = Scalar(self.value * other.value, (self, other), '*')
        
        def _backward():
            self.grad += other.value * out.grad
            other.grad += self.value * out.grad
        out._backward = _backward
        
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()
            
    def tanh(self):
        x = self.value
        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)
        out = Scalar(t, (self,), 'tanh')
        
        def _backward():
            self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        
        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float)), "only supporting int/float powers for now"
        out = Scalar(self.value ** other, (self,), f'**{other}')
        
        def _backward():
            self.grad += (other * self.value**(other-1)) * out.grad
        out._backward = _backward
        
        return out

import random
import math

class Neuron:
    def __init__(self, nin):
        self.w = [Scalar(random.uniform(-1, 1)) for _ in range(nin)]
        self.b = Scalar(0)
    
    def __call__(self, x):
        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)
        out = act.tanh()
        return out

class Layer:
    def __init__(self, nin, nout):
        self.neurons = [Neuron(nin) for _ in range(nout)]
    
    def __call__(self, x):
        outs = [neuron(x) for neuron in self.neurons]
        return outs if len(outs) > 1 else outs[0]

class MLP:
    def __init__(self, nin, nouts):
        sz = [nin] + nouts
        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]
    
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
    
    def parameters(self):
        return [p for layer in self.layers for neuron in layer.neurons for p in neuron.w] + [neuron.b for layer in self.layers for neuron in layer.neurons]

# Sample dataset
data = [
    ([Scalar(2.0), Scalar(3.0)], Scalar(1.0)),
    ([Scalar(1.0), Scalar(2.0)], Scalar(0.0)),
    ([Scalar(2.0), Scalar(2.0)], Scalar(1.0)),
    ([Scalar(3.0), Scalar(3.0)], Scalar(0.0))
]

# Define the model
model = MLP(2, [4, 4, 1])

# Training loop
learning_rate = 0.01
for epoch in range(100):
    # Forward pass
    loss = Scalar(0)
    for x, y in data:
        pred = model(x)[0]
        loss += (pred - y) ** 2

    # Backward pass
    loss.backward()
    
    # Update weights
    for p in model.parameters():
        p.value -= learning_rate * p.grad

    # Reset gradients
    for p in model.parameters():
        p.grad = 0.0

    print(f"Epoch {epoch}: Loss = {loss.value}")
```

### Explanation

Scalar Class: This class implements a scalar value that supports automatic differentiation. It includes methods for basic arithmetic operations (__add__, __mul__, __pow__) and the tanh activation function (tanh). It also tracks the computational graph and computes gradients during backpropagation (backward).

Neuron Class: Represents a single neuron that computes a weighted sum of inputs and applies the tanh activation function.

Layer Class: Consists of multiple neurons and can process an input vector.

MLP Class: Defines a multi-layer perceptron (MLP) with multiple layers. It supports both single output neurons and multiple output neurons.

Training Loop: This loop trains the model on a simple dataset. It performs forward passes to compute the loss, backward passes to compute gradients, and updates the model parameters using gradient descent.

This example demonstrates the core principles of automatic differentiation and backpropagation in a neural network, implemented from scratch using Python.


Debug tips 
for lab2.py 
```python
    pred = model(x)[0]
TypeError: 'Scalar' object is not subscriptable
```
The error arises because model(x) returns a single Scalar object for a single output neuron, and you are trying to access it as an element of a list. To fix this, we'll adjust the code to handle both single output neurons and multiple output neurons properly.
update the context as lab2b.py

```python
   loss += (pred - y) ** 2
TypeError: unsupported operand type(s) for -: 'Scalar' and 'Scalar'
```

The error occurs because the Scalar class does not have the subtraction (__sub__) and exponentiation (__pow__) operations implemented. Let's add these operations to the Scalar class to handle subtraction and exponentiation correctly.

updated code as lab2c.py and add plot function for displat the MLP connection, but fail to succeed.

A better explaination can be found in the following url in a video

(deep learning part1)[https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&vd_source=1221087bdfb55490ea89f7c5edefb5ae]
(deep learning part2)[https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&vd_source=1221087bdfb55490ea89f7c5edefb5ae]
(deep learning part3p1)[https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.999.0.0&vd_source=1221087bdfb55490ea89f7c5edefb5ae]
(deep learning part3p2)[https://www.bilibili.com/video/BV16x411V7Qg?p=2]
