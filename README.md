# llm101n-gpt-generated

karpathy Andrej maybe too busy. Learn by doing, rather than waiting.

chatgpt-generated context for studying Large language model.

refer to the copied the directory context from https://github.com/karpathy/LLM101n.git

---

> What I cannot create, I do not understand. -Richard Feynman

In this course we will build a Storyteller AI Large Language Model (LLM). Hand in hand, you'll be able create, refine and illustrate [little stories](https://huggingface.co/datasets/roneneldan/TinyStories) with the AI. We are going to build everything end-to-end from basics to a functioning web app similar to ChatGPT, from scratch in Python, C and CUDA, and with minimal computer science prerequisits. By the end you should have a relatively deep understanding of AI, LLMs, and deep learning more generally.

Syllabus(`check in after verified`)

- [x] [Chapter 01](./ch01/ch01%20Bigram%20Language%20Model.md) Bigram Language Model (language modeling)

- [x] [Chapter 02](./ch02/ch02.md) Micrograd (machine learning, backpropagation)

- [x] [Chapter 03](./ch03/ch03.md) N-gram model (multi-layer perceptron, matmul, gelu)

- [ ] [Chapter 04](./ch04/ch04.md) Attention (attention, softmax, positional encoder)

- [ ] [Chapter 05](./ch05/ch05.md) Transformer (transformer, residual, layernorm, GPT-2)

- [ ] [Chapter 06](./ch06/ch06.md) Tokenization (minBPE, byte pair encoding)

- [ ] [Chapter 07](./ch07/ch07.md)  Optimization (initialization, optimization, AdamW)

- [ ] [Chapter 08](./ch08/ch08.md)  Need for Speed I: Device (device, CPU, GPU, ...)

- [ ] [Chapter 09](./ch09/ch09.md)  Need for Speed II: Precision (mixed precision training, fp16, bf16, fp8, ...)

- [ ] [Chapter 10](./ch10/ch10.md)  Need for Speed III: Distributed (distributed optimization, DDP, ZeRO)

- [ ] [Chapter 11](./ch11/ch11.md) Datasets (datasets, data loading, synthetic data generation)

- [ ] [Chapter 12](./ch12/ch12.md) Inference I: kv-cache (kv-cache)

- [ ] [Chapter 13](./ch13/ch13.md) Inference II: Quantization (quantization)

- [ ] [Chapter 14](./ch14/ch14.md) Finetuning I: SFT (supervised finetuning SFT, PEFT, LoRA, chat)

- [ ] [Chapter 15](./ch15/ch15.md) Finetuning II: RL (reinforcement learning, RLHF, PPO, DPO)

- [ ] [Chapter 16](./ch16/ch16.md) Deployment (API, web app)

- [ ][ Chapter 17](./ch17/ch17.md) Multimodal (VQVAE, diffusion transformer)

Appendix

Further topics to work into the progression above:

Programming languages: Assembly, C, Python

Data types: Integer, Float, String (ASCII, Unicode, UTF-8)

Tensor: shapes, views, strides, contiguous, ...

Deep Learning frameowrks: PyTorch, JAX

Neural Net Architecture: GPT (1,2,3,4), Llama (RoPE, RMSNorm, GQA), MoE, ...

Multimodal: Images, Audio, Video, VQVAE, VQGAN, diffusion


