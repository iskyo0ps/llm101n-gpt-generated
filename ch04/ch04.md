Attention mechanisms, particularly self-attention, are key components of modern deep learning models, such as the Transformer architecture used in NLP tasks. Let's break down the components of attention mechanisms, including the attention mechanism itself, softmax for normalizing attention scores, and positional encodings for injecting order information into sequence data.

### Key Concepts

1. **Attention Mechanism**
2. **Softmax**
3. **Positional Encoding**

### 1. Attention Mechanism

The attention mechanism allows a model to focus on different parts of the input sequence when producing an output. In the context of self-attention used in Transformers, it helps to weigh the importance of different tokens in the sequence.

#### Scaled Dot-Product Attention

The scaled dot-product attention mechanism can be formulated as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q$: Query matrix
- $K$: Key matrix
- $V$: Value matrix
- $d_k$: Dimension of the key vectors (scaling factor)

### 2. Softmax

Softmax is used to convert the raw attention scores into probabilities, making sure they sum up to 1.

### 3. Positional Encoding

Since the attention mechanism itself does not inherently consider the order of the sequence, positional encodings are added to the input embeddings to provide this information.

### Implementation

Let's implement these components step-by-step in Python using PyTorch.

#### Scaled Dot-Product Attention

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights

# Example
Q = torch.rand(1, 5, 64)  # (batch_size, seq_len, d_k)
K = torch.rand(1, 5, 64)
V = torch.rand(1, 5, 64)

output, attention_weights = scaled_dot_product_attention(Q, K, V)
print("Output:", output)
print("Attention Weights:", attention_weights)
```

#### Positional Encoding

Positional encoding injects information about the position of each token in the sequence. Here, we use the sine and cosine functions of different frequencies as described in the original Transformer paper.

```python
import numpy as np
import torch.nn as nn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))
        self.encoding[:, 0::2] = torch.sin(position * div_term)
        self.encoding[:, 1::2] = torch.cos(position * div_term)
        self.encoding = self.encoding.unsqueeze(0)
        
    def forward(self, x):
        return x + self.encoding[:, :x.size(1), :]

# Example
d_model = 64
pos_encoder = PositionalEncoding(d_model)
input_tensor = torch.zeros(1, 5, d_model)
encoded_tensor = pos_encoder(input_tensor)
print("Positional Encoded Tensor:", encoded_tensor)
```

### Putting It All Together

Let's combine these components into a simple self-attention layer with positional encoding.

```python
import torch.nn as nn
import torch.optim as optim

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()
        self.d_model = d_model
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.positional_encoding = PositionalEncoding(d_model)
    
    def forward(self, x):
        x = self.positional_encoding(x)
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)
        return attention_output, attention_weights

# Example
seq_len = 5
d_model = 64
x = torch.rand(1, seq_len, d_model)

self_attention = SelfAttention(d_model)
output, attention_weights = self_attention(x)
print("Self-Attention Output:", output)
print("Attention Weights:", attention_weights)
```

### Explanation

1. **Attention Mechanism**:
   - The `scaled_dot_product_attention` function computes the attention scores and applies the softmax function to get attention weights.
   - These weights are used to compute a weighted sum of the values, producing the attention output.

2. **Positional Encoding**:
   - The `PositionalEncoding` class adds positional information to the input embeddings using sine and cosine functions.
   - This encoding is added to the input tensor in the forward pass of the network.

3. **Self-Attention Layer**:
   - The `SelfAttention` class integrates the positional encoding and the attention mechanism.
   - It includes linear layers to project the input into query, key, and value spaces.
   - The forward method computes the attention output using these projections.

This code demonstrates how to implement attention, softmax, and positional encoding from scratch, providing a foundation for building more complex models like Transformers.